{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML & NLP a brief account\n",
    "### - The roots\n",
    "### - Embeddings\n",
    "### - Recurrent Neural Networks\n",
    "### - Attention is all you need\n",
    "### - Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The roots\n",
    "\n",
    "\n",
    "\n",
    "### - Vector space model\n",
    "\n",
    "### - Latent Semantic analysis / Latent Dirichlet Analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The vector space model\n",
    "It all started with Salton in 1975 and his vector space model\n",
    "\n",
    "![vsm](Vector_space_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSA & LDA\n",
    "The main limitation of the Vector Space Model lies in its incapacity to detect close documents which do not have common terms. For instance : \"this is a superb kitty\" and \"it's a beautiful cat\"\n",
    "\n",
    "![lsalda](LSA-LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Embeddings\n",
    "One can see them as the idea of dimension reduction of the vector space model to highlight semantic proximities by other means. [A good Medium article](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "\n",
    "### CBOW\n",
    "![lsalda](CBOW.png) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "### SkipGram\n",
    "Almost the reverse model\n",
    "![lsalda](SkipGram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateFood.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateWeather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateCookingSchedule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateVectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateRNNPie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateRNNBurger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateRNNChicken.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateRNNSunny.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateRNNCloudy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateMergingFoodAndWeather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "![rnn](RoomMateMergingFinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "**Food output is reinjected in the network as a state vector**\n",
    "![rnn](RoomMateRNNFinalNetwork.png)\n",
    "\n",
    "**These slides on the perfect roommate are borrowed from this [Luis Serrano's Youtube video](https://www.youtube.com/watch?v=BR9h47Jtqyw)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing and exploding gradient\n",
    "![lr](learningRate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing and exploding gradient\n",
    "![lr](VanishingAndExploding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different types of RNN\n",
    "![lr](DifferentTypesOfRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![aiayn](Decodeur.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![lr](AttentionRNN.png)\n",
    "These slides come from this [Ömer Sakarya paper](https://towardsdatascience.com/introduction-to-rnns-sequence-to-sequence-language-translation-and-attention-fc43ef2cc3fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![lr](AttentionBiDirRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![lr](SelfAttention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![lr](SelfAttentionDetailes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![lr](AlignmentMatrix.png)\n",
    "\n",
    "These last eight slides are borrowed from this [Lilian Weng's article](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![attention](attention.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the origin)\n",
    "![attention](attention.png)\n",
    "\n",
    "**These last two slides are borrowed from this [Google AI's blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty transformer)\n",
    "![attention](TransformerSummary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty transformer)\n",
    "![attention](The_transformer_encoders_decoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](The_transformer_encoder_decoder_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](Transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](Transformer_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](Transformer_decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](encoder_with_tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](self-attention-matrix-calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_attention_heads_qkv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_attention_heads_z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_positional_encoding_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_resideual_layer_norm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_decoding_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention is all you need (the mighty tranformer)\n",
    "\n",
    "![attention](transformer_decoder_output_softmax.png)\n",
    "**All the slides on the transformer above are borrowed from this [Jay Alammar's brillant article](http://jalammar.github.io/illustrated-transformer/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](openai-transformer-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](openai-transformer-language-modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](openai-transformer-sentence-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](openai-transformer-sentence-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](openai-input-transformations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](BERT-language-modeling-masked-lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](bert-next-sentence-prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "![applications](bert-tasks.png)\n",
    "**All the slides on the applications of transformer above are borrowed from this [Jay Alammar's article](http://jalammar.github.io/illustrated-bert/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GPT-2\n",
    "SYSTEM PROMPT (HUMAN-WRITTEN)\n",
    "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "MODEL COMPLETION (MACHINE-WRITTEN, 10 TRIES)\n",
    "The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    "Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    "Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    "Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n",
    "\n",
    "While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n",
    "Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n",
    "\n",
    "While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n",
    "\n",
    "However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQuAD 2.0\n",
    "\n",
    "Let's explore [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![thatsall](ThatsAllFolks.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
