{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary dependencies\n",
    "import os, random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet.ndarray as F\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cpu context, if using cpu mx.gpu(0)\n",
    "context= mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '../../../Corpus/lch-mini.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(source, length=None, limit=None,):\n",
    "    \"\"\"\n",
    "    Reads a text file as a stream of characters. The stream is cut into chunks of equal size\n",
    "    :param source: The text file to read\n",
    "    :param length: The size of the chunks. If None, the stream is delimited by line-ends and the resulting sequence will\n",
    "        have variable length\n",
    "    :param limit: If not None, only the first \"character_limit\" characters are read. Useful for debugging on large corpora.\n",
    "    :return: (1) A list of lists\n",
    "    \"\"\"\n",
    "\n",
    "    # Reading raw text from source and destination files\n",
    "    f = open(source, 'r')\n",
    "    x_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "    print('raw data read')\n",
    "    return x_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data read\n",
      "total chars: 106\n",
      "\n",
      " !'(),-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{}«»ÇÈÉÊàâæçèéêëîïôù\n",
      "1162634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Online Distributed Proofreading Team at http: //www. pgdp. net ( This'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = load_corpus(corpus)\n",
    "# total of characters in dataset\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1 # We need extra character for padding\n",
    "print('total chars:', vocab_size)\n",
    "\n",
    "#zeros for padding\n",
    "chars.insert(0, \"\\0\") # We need extra character for padding the sequenece\n",
    "\n",
    "#Display characters in dataset\n",
    "print(''.join(chars[1:-6]))\n",
    "\n",
    "\n",
    "# maps character to unique index e.g. {a:1,b:2....}\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "# maps indices to character (1:a,2:b ....)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# mapping the dataset into index\n",
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "print(len(idx))\n",
    "#testing the mapping\n",
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162634\n",
      "227653\n",
      "27902\n",
      "2831\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "\n",
    "words = text.split(\" \")\n",
    "print(len(words))\n",
    "\n",
    "voc = sorted(list(set(words)))\n",
    "print(len(voc))\n",
    "\n",
    "paragraphs = text.split(\"\\n\")\n",
    "print(len(paragraphs))\n",
    "\n",
    "books = text.split(\"\\n\\n\")\n",
    "print(len(books))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387542, 3)\n"
     ]
    }
   ],
   "source": [
    "# input for neural network(our basic rnn has 3 inputs, n samples)\n",
    "cs = 3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "# the output of rnn network (single vector)\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]\n",
    "\n",
    "# stacking the inputs to form (3 input features )\n",
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])\n",
    "\n",
    "# the output (1 X N data points)\n",
    "y = np.stack(c4_dat[:-2])\n",
    "\n",
    "col_concat = np.array([x1, x2, x3])\n",
    "t_col_concat = col_concat.T\n",
    "print(t_col_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used for testing purpose\n",
    "# you can pass this input to the neural network and see if the shapes are right\n",
    "# our sample inputs for the model\n",
    "x1_nd = mx.nd.array(x1)\n",
    "x2_nd = mx.nd.array(x2)\n",
    "x3_nd = mx.nd.array(x3)\n",
    "sample_input = mx.nd.array([ [x1[0],x2[0],x3[0]] ,[x1[1],x2[1],x3[1] ] ])\n",
    "\n",
    "simple_train_data = mx.nd.array(t_col_concat)\n",
    "simple_label_data = mx.nd.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# Set the batchsize as 32, so input is of form 32 X 3\n",
    "# output is 32 X 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_batch(source, label_data, i, batch_size=32):\n",
    "    bb_size = min(batch_size, source.shape[0] - 1 - i)\n",
    "    data = source[i: i + bb_size]\n",
    "    target = label_data[i: i + bb_size]\n",
    "    # print(target.shape)\n",
    "    return data, target.reshape((-1, ))\n",
    "\n",
    "test_bat,test_target = get_batch(simple_train_data,simple_label_data,5,batch_size)\n",
    "print(test_bat.shape)\n",
    "print(test_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output shape (2, 107)\n"
     ]
    }
   ],
   "source": [
    "#simple UnRollredRNN_Model\n",
    "from mxnet.gluon import Block, nn\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "class UnRolledRNN_Model(Block):\n",
    "    def __init__(self,vocab_size, num_embed, num_hidden,**kwargs):\n",
    "        super(UnRolledRNN_Model, self).__init__(**kwargs)\n",
    "        self.num_embed = num_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # use name_scope to give child Blocks appropriate names.\n",
    "        # It also allows sharing Parameters between Blocks recursively.\n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "            self.dense1 = nn.Dense(num_hidden,activation='relu',flatten=True)\n",
    "            self.dense2 = nn.Dense(num_hidden,activation='relu',flatten=True)\n",
    "            self.dense3 = nn.Dense(vocab_size,flatten=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emd = self.encoder(inputs)\n",
    "        #print(emd.shape)\n",
    "        #since the input is shape(batch_size,input(3 characters))\n",
    "        # we need to extract 0th,1st,2nd character from each batch\n",
    "        chararcter1 = emd[:,0,:]\n",
    "        chararcter2 = emd[:,1,:]\n",
    "        chararcter3 = emd[:,2,:]\n",
    "        c1_hidden = self.dense1(chararcter1) # green arrow in diagram for character 1\n",
    "        c2_hidden = self.dense1(chararcter2) # green arrow in diagram for character 2\n",
    "        c3_hidden = self.dense1(chararcter3) # green arrow in diagram for character 3\n",
    "        c1_hidden_2 = self.dense2(c1_hidden)  # yellow arrow in diagram\n",
    "        addition_result = F.add(c2_hidden,c1_hidden_2) # Total c1 + c2\n",
    "        addition_hidden = self.dense2(addition_result) # the yellow arrow\n",
    "        addition_result_2 = F.add(addition_hidden,c3_hidden) # Total c1 + c2\n",
    "        final_output = self.dense3(addition_result_2)      \n",
    "        return final_output\n",
    "    \n",
    "vocab_size = len(chars)+1 # the vocabsize, extra character for zero padding\n",
    "num_embed = 30\n",
    "num_hidden = 256\n",
    "#model creatings\n",
    "simple_model = UnRolledRNN_Model(vocab_size, num_embed, num_hidden)\n",
    "#model initilisation\n",
    "simple_model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(simple_model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "#sample input shape is of size (2x3)\n",
    "output = simple_model(sample_input)\n",
    "#sample out shape should be(3*87). 87 is our vocab size\n",
    "print('the output shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point file\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "filename_unrolled_rnn = \"checkpoints/rnn_gluon_abc.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual training \n",
    "def UnRolledRNNtrain(train_data,label_data,batch_size=32,epochs=10):\n",
    "    epochs = epochs\n",
    "    smoothing_constant = .01\n",
    "    for e in range(epochs):\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, batch_size)):\n",
    "            data, target = get_batch(train_data,label_data, i,batch_size)\n",
    "            data = data.as_in_context(context)\n",
    "            target = target.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                output = simple_model(data)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if ibatch == 128:\n",
    "                curr_loss = mx.nd.mean(L).asscalar()\n",
    "                moving_loss = 0\n",
    "                moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                           else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "                print(\"Epoch %s. Loss: %s, moving_loss %s\" % (e,curr_loss,moving_loss))   \n",
    "    simple_model.save_params(filename_unrolled_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.9099426, moving_loss 0.02909942626953125\n",
      "Epoch 1. Loss: 1.8340509, moving_loss 0.018340508937835693\n",
      "Epoch 2. Loss: 1.823588, moving_loss 0.018235880136489867\n",
      "Epoch 3. Loss: 1.7600121, moving_loss 0.01760012149810791\n",
      "Epoch 4. Loss: 1.736971, moving_loss 0.017369710206985474\n",
      "Epoch 5. Loss: 1.750953, moving_loss 0.01750952959060669\n",
      "Epoch 6. Loss: 1.7382294, moving_loss 0.017382293939590454\n",
      "Epoch 7. Loss: 1.7327875, moving_loss 0.017327874898910522\n",
      "Epoch 8. Loss: 1.7566776, moving_loss 0.017566776275634768\n",
      "Epoch 9. Loss: 1.7498496, moving_loss 0.017498495578765868\n"
     ]
    }
   ],
   "source": [
    "#let us train the model\n",
    "epochs = 10\n",
    "UnRolledRNNtrain(simple_train_data,simple_label_data,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model back\n",
    "simple_model.load_params(filename_unrolled_rnn, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model\n",
    "def evaluate(input_string):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    sample_input = mx.nd.array([[ idx[0],idx[1],idx[2] ]],ctx=context)\n",
    "    output = simple_model(sample_input)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    return index.asnumpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted answer is  i\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "begin_char = 'mar'\n",
    "answer = evaluate(begin_char)\n",
    "print('the predicted answer is ',indices_char[answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN using gluon/lstm api\n",
    "Training sequence 2 sequence models using Gluon API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxnet.gluon import  nn\n",
    "import gluonnlp as nlp\n",
    "\n",
    "import mxnet.gluon.rnn as rnn\n",
    "\n",
    "\n",
    "\n",
    "class GluonRNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, **kwargs):\n",
    "        super(GluonRNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "\n",
    "            if mode == 'lstm':\n",
    "                #  we create a LSTM layer with certain number of hidden LSTM cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the LSTM will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                #  we create a GRU layer with certain number of hidden GRU cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the GRU will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                #  we create a vanilla RNN layer with certain number of hidden vanilla RNN cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the vanilla will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            self.decoder = nn.Dense(vocab_size, in_units=num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    #  define the forward pass of the neural network\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.encoder(inputs)\n",
    "        #  emb, hidden are the inputs to the hidden \n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        #  the ouput from the hidden layer to passed to drop out layer\n",
    "        output = self.drop(output)\n",
    "        #  print('output forward',output.shape)\n",
    "        #  Then the output is flattened to a shape for the dense layer  \n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    # Initial state of RNN layer\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lstm\n",
    "mode = 'lstm'\n",
    "vocab_size = len(chars)+1 # number of characters in vocab_size including the zero padding\n",
    "embedsize = 500\n",
    "hididen_units = 1000\n",
    "number_layers = 2\n",
    "clip = 0.2\n",
    "epochs = 2 # use 200 epochs for good result\n",
    "batch_size = 32\n",
    "seq_length = 100 # sequence length\n",
    "dropout = 0.4\n",
    "log_interval = 64\n",
    "rnn_save = 'checkpoints/gluonlstm_abc' #checkpoints/gluonlstm_2 (prepared for seq_lenght 100, 200 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares rnn batches\n",
    "# The batch will be of shape is (num_example * batch_size) because of RNN uses sequences of input     x\n",
    "# for example if we use (a1,a2,a3) as one input sequence , (b1,b2,b3) as another input sequence and (c1,c2,c3)\n",
    "# if we have batch of 3, then at timestep '1'  we only have (a1,b1.c1) as input, at timestep '2' we have (a2,b2,c2) as input...\n",
    "# hence the batchsize is of order \n",
    "# In feedforward we use (batch_size, num_example)\n",
    "def rnn_batch(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GluonRNNModel \n",
    "model = GluonRNNModel(mode, vocab_size, embedsize, hididen_units,\n",
    "                       number_layers, dropout)\n",
    "# initalise the weights of models to random weights\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "# Adam trainer\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam')\n",
    "#softmax cros entropy loss\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares rnn batches\n",
    "# The batch will be of shape is (num_example * batch_size) because of RNN uses sequences of input\n",
    "# for example if we use (a1,a2,a3) as one input sequence , (b1,b2,b3) as another input sequence and (c1,c2,c3)\n",
    "# if we have batch of 3, then at timestep '1'  we only have (a1,b1.c1) as input, at timestep '2' we have (a2,b2,c2) as input...\n",
    "# hence the batchsize is of order (num_example * batch_size) \n",
    "# In feedforward we use (batch_size, num_example)\n",
    "def rnn_batch(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "idx_nd = mx.nd.array(idx)\n",
    "# convert the idex of characters\n",
    "train_data_rnn_gluon = rnn_batch(idx_nd, batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the batch\n",
    "def get_batch(source, i,seq):\n",
    "    seq_len = min(seq, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "# detach the hidden state, so we dont accidentally compute gradients\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGluonRNN(epochs, train_data, seq=seq_length):\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, seq_length)):\n",
    "            data, target = get_batch(train_data, i, seq)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target) # this is total loss associated with seq_length\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and seq_length to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, clip * seq_length * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / seq_length / batch_size / log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f'%(epoch + 1, ibatch, cur_L))\n",
    "                total_L = 0.0\n",
    "        model.save_params(rnn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train data shape is (36332, 32)\n"
     ]
    }
   ],
   "source": [
    "print('the train data shape is',train_data_rnn_gluon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 64] loss 2.09\n",
      "[Epoch 1 Batch 128] loss 1.79\n",
      "[Epoch 1 Batch 192] loss 1.63\n",
      "[Epoch 1 Batch 256] loss 1.55\n",
      "[Epoch 1 Batch 320] loss 1.46\n",
      "[Epoch 2 Batch 64] loss 1.40\n",
      "[Epoch 2 Batch 128] loss 1.33\n",
      "[Epoch 2 Batch 192] loss 1.30\n",
      "[Epoch 2 Batch 256] loss 1.29\n",
      "[Epoch 2 Batch 320] loss 1.26\n"
     ]
    }
   ],
   "source": [
    "trainGluonRNN(epochs,train_data_rnn_gluon,seq=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_params(rnn_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluates a seqtoseq model over the input string\n",
    "#The output generated at each instance is captured eg. at t1->t2, t1,t2->t3, t1,t2,t3 -> t4 \n",
    "def evaluate_seq2seq(model,input_string,seq_length,batch_size):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    if(len(input_string) != seq_length):\n",
    "        raise ValueError(\"input string should be equal to sequence length\")\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size, ctx=context)\n",
    "    sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T\n",
    "                                ,ctx=context)\n",
    "    output,hidden = model(sample_input,hidden)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    index = index.asnumpy()\n",
    "    return [indices_char[char] for char in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps the input sequence to output sequence\n",
    "def mapInput(input_str,output_str):\n",
    "    for i,_ in enumerate(input_str):\n",
    "        partial_input = input_str[:i+1]\n",
    "        partial_output = output_str[i:i+1]\n",
    "        print(partial_input + \"->\" + partial_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "A->n\n",
      "A ->d\n",
      "A l->e\n",
      "A l'-> \n",
      "A l' ->a\n",
      "A l' é->t\n",
      "A l' ép->r\n",
      "A l' épo->q\n",
      "A l' époq->u\n",
      "A l' époqu->e\n",
      "A l' époque-> \n",
      "A l' époque ->d\n",
      "A l' époque o->ù\n",
      "A l' époque où-> \n",
      "A l' époque où ->i\n",
      "A l' époque où c->e\n",
      "A l' époque où co->m\n",
      "A l' époque où com->m\n",
      "A l' époque où comm->e\n",
      "A l' époque où comme-> \n",
      "A l' époque où commen->c\n",
      "A l' époque où commenc->e\n",
      "A l' époque où commence-> \n",
      "A l' époque où commence ->d\n",
      "A l' époque où commence c->o\n",
      "A l' époque où commence ce-> \n",
      "A l' époque où commence cet->t\n",
      "A l' époque où commence cett->e\n",
      "A l' époque où commence cette-> \n",
      "A l' époque où commence cette ->f\n",
      "A l' époque où commence cette h->o\n",
      "A l' époque où commence cette hi->s\n",
      "A l' époque où commence cette his->t\n",
      "A l' époque où commence cette hist->o\n",
      "A l' époque où commence cette histo->i\n",
      "A l' époque où commence cette histoi->r\n",
      "A l' époque où commence cette histoir->e\n",
      "A l' époque où commence cette histoire-> \n",
      "A l' époque où commence cette histoire,-> \n",
      "A l' époque où commence cette histoire, -> \n",
      "A l' époque où commence cette histoire,  ->e\n",
      "A l' époque où commence cette histoire,  l->e\n",
      "A l' époque où commence cette histoire,  la-> \n",
      "A l' époque où commence cette histoire,  la ->p\n",
      "A l' époque où commence cette histoire,  la p->o\n",
      "A l' époque où commence cette histoire,  la pr->o\n",
      "A l' époque où commence cette histoire,  la pre->m\n",
      "A l' époque où commence cette histoire,  la pres->s\n",
      "A l' époque où commence cette histoire,  la press->e\n",
      "A l' époque où commence cette histoire,  la presse-> \n",
      "A l' époque où commence cette histoire,  la presse ->d\n",
      "A l' époque où commence cette histoire,  la presse d->e\n",
      "A l' époque où commence cette histoire,  la presse de-> \n",
      "A l' époque où commence cette histoire,  la presse de ->l\n",
      "A l' époque où commence cette histoire,  la presse de S->é\n",
      "A l' époque où commence cette histoire,  la presse de St->a\n",
      "A l' époque où commence cette histoire,  la presse de Sta->n\n",
      "A l' époque où commence cette histoire,  la presse de Stan->i\n",
      "A l' époque où commence cette histoire,  la presse de Stanh->o\n",
      "A l' époque où commence cette histoire,  la presse de Stanho->u\n",
      "A l' époque où commence cette histoire,  la presse de Stanhop->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope ->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope e->t\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et ->l\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et l->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et le->s\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les ->p\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les r->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les ro->m\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rou->l\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les roul->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les roule->a\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les roulea->u\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleau->x\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux ->d\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à ->l\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à d->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à di->x\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à dis->p\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à dist->i\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distr->a\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distri->c\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distrib->u\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribu->t\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribue->r\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer ->l\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l'-> \n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' ->a\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' e->s\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' en->f\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' enc->o\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' encr->e\n",
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' encre-> \n"
     ]
    }
   ],
   "source": [
    "test_input = \"A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' encre ne fonctionnaient pas encore dans les petites imprimeries de provinces.\"\n",
    "test_input = test_input[:100]\n",
    "print(len(test_input))\n",
    "print(len(test_input))\n",
    "result= evaluate_seq2seq(model,test_input,seq_length,1)\n",
    "mapInput(test_input,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un générateur à la mode de La comédie humaine\n",
    "import sys\n",
    "def generate_random_text(model,input_string,seq_length,batch_size,sentence_length):\n",
    "    count = 0\n",
    "    new_string = ''\n",
    "    cp_input_string = input_string\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = batch_size, ctx=context)\n",
    "    while count < sentence_length:\n",
    "        idx = [char_indices[c] for c in input_string]\n",
    "        if(len(input_string) != seq_length):\n",
    "            print(len(input_string))\n",
    "            raise ValueError('there was a error in the input ')\n",
    "        sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T\n",
    "                                ,ctx=context)\n",
    "        output,hidden = model(sample_input,hidden)\n",
    "        index = mx.nd.argmax(output, axis=1)\n",
    "        index = index.asnumpy()\n",
    "        count = count + 1\n",
    "        new_string = new_string + indices_char[index[-1]]\n",
    "        input_string = input_string[1:] + indices_char[index[-1]]\n",
    "    print(cp_input_string + new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A l' époque où commence cette histoire,  la presse de Stanhope et les rouleaux à distribuer l' encre de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la p\n"
     ]
    }
   ],
   "source": [
    "generate_random_text(model,test_input,seq_length,1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
